{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba52a858-ef1d-416b-a6a8-29eddc96a44a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3422, 856]\n"
     ]
    }
   ],
   "source": [
    "## 导入包\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Subset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler # 多数样本下采样\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 加载数据\n",
    "data = np.load(\"./data_processed/data_norm.npy\") # 30502个样本\n",
    "labels = np.load(\"./data_processed/labels.npy\")  # 2139个负样本1，其余全是正样本0\n",
    "\n",
    "# 划分训练集测试集\n",
    "train_sever_id = np.load('train_server_id.npy')\n",
    "test_sever_id = np.load('test_server_id.npy')\n",
    "\n",
    "# 将data和labels转换为 PyTorch 张量\n",
    "data_tensor = torch.tensor(data, dtype = torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype = torch.long)\n",
    "# 创建 TensorDataset\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "\n",
    "# 使用 Subset 根据索引创建训练集和测试集\n",
    "train_dataset = Subset(dataset, train_sever_id)\n",
    "test_dataset = Subset(dataset, test_sever_id)\n",
    "\n",
    "\n",
    "train_size, test_size = len(train_dataset), len(test_dataset)\n",
    "print([train_size, test_size])\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = test_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c99e8e06-44f1-40a9-9f95-87db35d4f410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1): # 指定输入通道，输出通道\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 输入是 16x32x16\n",
    "class ResNet_Encoder(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(ResNet_Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(16, 4, kernel_size=3, stride=1, padding=1) # 输入为16个通道\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.layer1 = self._make_layer(4, 8, 2, stride=2) # 输入通道、输出通道、块数、步长\n",
    "        self.layer2 = self._make_layer(8, 16, 2, stride=1)\n",
    "        # self.layer3 = self._make_layer(32, 64, 2, stride=1)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=16, nhead=8), # 两个参数分别是输入样本的维度(32)和Encoder头的数量(头的数量必须能被维度整除)\n",
    "            num_layers= 4, # 层数，即最终的Encoder由4个EncoderLayer组成\n",
    "        ) # 最终transformer_encoder的输入形式是(序列长度，批大小，维度)，输出也还是(序列长度，批大小，维度)，大小都不变\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*128, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 经过残差块\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.flatten(x, 2) # 把特征图拉平,通道留下,[1,32,32] [批大小,维度(通道数),序列长度]\n",
    "        \n",
    "        # 经过TransformerEncoder\n",
    "        x = x.permute(2, 0, 1) # 更改为[序列长度,批大小,维度]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 2, 0) # 更改为[批大小,维度,序列长度]\n",
    "        \n",
    "        # 全连接分类\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x) # 这层的输出作为特征\n",
    "        x = self.fc2(features)\n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47cebf05-96c3-4995-a3db-da7de2af600b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet_Encoder                                [1, 2]                    --\n",
       "├─Conv2d: 1-1                                 [1, 4, 32, 16]            580\n",
       "├─BatchNorm2d: 1-2                            [1, 4, 32, 16]            8\n",
       "├─ReLU: 1-3                                   [1, 4, 32, 16]            --\n",
       "├─Sequential: 1-4                             [1, 8, 16, 8]             --\n",
       "│    └─ResidualBlock: 2-1                     [1, 8, 16, 8]             --\n",
       "│    │    └─Conv2d: 3-1                       [1, 8, 16, 8]             296\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 8, 16, 8]             16\n",
       "│    │    └─ReLU: 3-3                         [1, 8, 16, 8]             --\n",
       "│    │    └─Conv2d: 3-4                       [1, 8, 16, 8]             584\n",
       "│    │    └─BatchNorm2d: 3-5                  [1, 8, 16, 8]             16\n",
       "│    │    └─Sequential: 3-6                   [1, 8, 16, 8]             56\n",
       "│    │    └─ReLU: 3-7                         [1, 8, 16, 8]             --\n",
       "│    └─ResidualBlock: 2-2                     [1, 8, 16, 8]             --\n",
       "│    │    └─Conv2d: 3-8                       [1, 8, 16, 8]             584\n",
       "│    │    └─BatchNorm2d: 3-9                  [1, 8, 16, 8]             16\n",
       "│    │    └─ReLU: 3-10                        [1, 8, 16, 8]             --\n",
       "│    │    └─Conv2d: 3-11                      [1, 8, 16, 8]             584\n",
       "│    │    └─BatchNorm2d: 3-12                 [1, 8, 16, 8]             16\n",
       "│    │    └─Sequential: 3-13                  [1, 8, 16, 8]             --\n",
       "│    │    └─ReLU: 3-14                        [1, 8, 16, 8]             --\n",
       "├─Sequential: 1-5                             [1, 16, 16, 8]            --\n",
       "│    └─ResidualBlock: 2-3                     [1, 16, 16, 8]            --\n",
       "│    │    └─Conv2d: 3-15                      [1, 16, 16, 8]            1,168\n",
       "│    │    └─BatchNorm2d: 3-16                 [1, 16, 16, 8]            32\n",
       "│    │    └─ReLU: 3-17                        [1, 16, 16, 8]            --\n",
       "│    │    └─Conv2d: 3-18                      [1, 16, 16, 8]            2,320\n",
       "│    │    └─BatchNorm2d: 3-19                 [1, 16, 16, 8]            32\n",
       "│    │    └─Sequential: 3-20                  [1, 16, 16, 8]            176\n",
       "│    │    └─ReLU: 3-21                        [1, 16, 16, 8]            --\n",
       "│    └─ResidualBlock: 2-4                     [1, 16, 16, 8]            --\n",
       "│    │    └─Conv2d: 3-22                      [1, 16, 16, 8]            2,320\n",
       "│    │    └─BatchNorm2d: 3-23                 [1, 16, 16, 8]            32\n",
       "│    │    └─ReLU: 3-24                        [1, 16, 16, 8]            --\n",
       "│    │    └─Conv2d: 3-25                      [1, 16, 16, 8]            2,320\n",
       "│    │    └─BatchNorm2d: 3-26                 [1, 16, 16, 8]            32\n",
       "│    │    └─Sequential: 3-27                  [1, 16, 16, 8]            --\n",
       "│    │    └─ReLU: 3-28                        [1, 16, 16, 8]            --\n",
       "├─TransformerEncoder: 1-6                     [128, 1, 16]              --\n",
       "│    └─ModuleList: 2-5                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-29     [128, 1, 16]              68,752\n",
       "│    │    └─TransformerEncoderLayer: 3-30     [128, 1, 16]              68,752\n",
       "│    │    └─TransformerEncoderLayer: 3-31     [128, 1, 16]              68,752\n",
       "│    │    └─TransformerEncoderLayer: 3-32     [128, 1, 16]              68,752\n",
       "├─Linear: 1-7                                 [1, 16]                   32,784\n",
       "├─Linear: 1-8                                 [1, 2]                    34\n",
       "===============================================================================================\n",
       "Total params: 319,014\n",
       "Trainable params: 319,014\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 36.30\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 8.86\n",
       "Params size (MB): 1.26\n",
       "Estimated Total Size (MB): 10.16\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet_Encoder().to(device) # 实例化\n",
    "\n",
    "summary(model, input_size=(1,16,32,16)) # 可视化网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f217f27f-35de-4078-a945-a78165261b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 训练模型\n",
    "criterion = nn.CrossEntropyLoss() # 损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(x_train) # 训练的时候不需要获得features\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计训练集的损失\n",
    "        running_loss += loss.item() * x_train.size(0)\n",
    "        \n",
    "        # 统计训练集的准确率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_train).sum().item()\n",
    "\n",
    "    # 计算每个 epoch 的训练损失和准确率\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_accuracy = 100 * correct / train_size\n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == num_epochs-1 :\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%')\n",
    "    \n",
    "        ##------------ 每轮进行一次测试-----------\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                outputs, _ = model(x_test)\n",
    "                _, y_pred = torch.max(outputs, 1)\n",
    "                test_correct += (y_pred == y_test).sum().item()\n",
    "\n",
    "        test_accuracy = 100 * test_correct / test_size\n",
    "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "        \n",
    "\n",
    "## 直接测试\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_loader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        outputs, _ = model(x_test)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        test_correct += (y_pred == y_test).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_size\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "y_test, y_pred = y_test.cpu(), y_pred.cpu()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 显示混淆矩阵\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "## 计算Recall、pre\n",
    "precision = precision_score(y_test, y_pred, pos_label = 1) # 指定阳性样本（正样本）\n",
    "recall = recall_score(y_test, y_pred, pos_label = 1)\n",
    "print([f\"pre:\", precision, f\"recall:\", recall])\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model, 'trainedmodel.pth') \n",
    "net = torch.load('trainedmodel.pth') # 加载模型\n",
    "\n",
    "model = model.to('cpu') # 迁移到CPU，防止报cuda内存不足\n",
    "## 用训练好的模型提取特征\n",
    "train_loader2 = DataLoader(train_dataset, batch_size=len(train_dataset) ,shuffle = None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_train, y_train in train_loader2:\n",
    "        # x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        _, train_features = model(x_train) # train_features为 样本数*16的矩阵\n",
    "        train_label = y_train.cpu().numpy() # train_label转化为数组\n",
    "\n",
    "    for x_test, y_test in test_loader:\n",
    "        # x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        _, test_features = model(x_test)\n",
    "        test_label = y_test.cpu().numpy()\n",
    "\n",
    "# 创建SVM模型\n",
    "svm_model = SVC(kernel='rbf')\n",
    "# CNN的输出为Tensor，需要转化为numpy输入给SVM\n",
    "# 训练SVM\n",
    "svm_model.fit(train_features, train_label)\n",
    "\n",
    "# 测试\n",
    "y_pred = svm_model.predict(test_features)\n",
    "\n",
    "# 计算模型准确率\n",
    "accuracy = np.mean(y_pred == test_label)\n",
    "print(f\"模型准确率: {100*accuracy:.2f} %\")\n",
    "\n",
    "# 可视化分类结果\n",
    "acc = accuracy_score(test_label,y_pred) # acc\n",
    "pre = precision_score(test_label,y_pred) # pre\n",
    "recall = recall_score(test_label,y_pred) # reacall\n",
    "f1score = f1_score(test_label,y_pred) # f1-socre\n",
    "print('计算指标结果：\\nAcc: %.2f%% \\nPre: %.2f%% \\nRecall: %.2f%% \\nF1-score: %.2f%% ' % (100*acc,100*pre,100*recall,100*f1score))\n",
    "\n",
    "## 绘制混淆矩阵\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_label,y_pred))\n",
    "disp.plot()\n",
    "\n",
    "\n",
    "## 保存网络提取的特征和标签\n",
    "np.save('train_features.npy', train_features)\n",
    "np.save('test_features.npy', test_features)\n",
    "\n",
    "np.save('train_label.npy',train_label)\n",
    "np.save('test_label.npy', test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ebc42ba-a3d2-42f6-9fbe-a7b3bfea4bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef29684-ed05-4d5b-bfe3-97ed906f464d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"trainedmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8311d-bcbc-43bc-bdbe-d88b12cf990b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2e9847-dcae-4d31-81f5-5134499f0547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichikage/anaconda3/envs/dram-py310/lib/python3.10/site-packages/torch/jit/_recursive.py:266: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
     ]
    }
   ],
   "source": [
    "torch.jit.save(torch.jit.script(model), \"ResNet_Transformer.pt\") # torch.jit.script会编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce9f4c-8345-4fb6-ae85-970dc57475db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dram-py310",
   "language": "python",
   "name": "dram-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
